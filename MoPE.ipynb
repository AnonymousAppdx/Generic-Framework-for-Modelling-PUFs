{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 264.51 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pypuf.simulation import *\n",
    "import pypuf.metrics as pm\n",
    "from pypuf.io import random_inputs\n",
    "import pypuf.io, pypuf.simulation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics, activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from mmoe import MMoE\n",
    "import datetime\n",
    "import psutil\n",
    "from tensorflow.python.profiler import profiler_client\n",
    "from preprocessing import *\n",
    "\n",
    "process = psutil.Process()\n",
    "memory_info_start = process.memory_info()\n",
    "\n",
    "# 打印内存使用情况\n",
    "print(f\"Memory: {memory_info_start.rss / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important callbacks and the MoPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDynamicThresholdCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, custom_layer, **kwargs):\n",
    "        super(SingleDynamicThresholdCallback, self).__init__(**kwargs)\n",
    "        self.custom_layer = custom_layer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_accuracy = logs.get('accuracy')\n",
    "        if current_accuracy>0.95:\n",
    "            self.model.stop_training = True\n",
    "            print(\"Early Stop!\")\n",
    "        elif current_accuracy>0.90:\n",
    "            K.set_value(self.custom_layer.threshold,0.001)\n",
    "        elif current_accuracy>0.8:\n",
    "            K.set_value(self.custom_layer.threshold,0.0005)\n",
    "        elif current_accuracy>0.7:\n",
    "            K.set_value(self.custom_layer.threshold,0.0001)\n",
    "\n",
    "\n",
    "                \n",
    "class MemoryUsageCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            process = psutil.Process()\n",
    "            memory_info = process.memory_info()\n",
    "            print(f\"Epoch {epoch + 1} - Memory Cost: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "memory_callback = MemoryUsageCallback()\n",
    "\n",
    "def Model_Multiple_PUFs(units_mmoe,gate_activation,N_train,train_c,test_c,train_r_groups,test_r_groups,PUF_list,units_tower=2):    \n",
    "    num_features = 64\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    experts_list = []\n",
    "    activation = 'relu'\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    drop_out_rate = 0.2\n",
    "    num_neu = 2**5\n",
    "\n",
    "    num_expert = 7\n",
    "    experts_list = [Expert_customize([num_neu,num_neu],activation=activation,kernel_init=kernel_init,drop_out_rate=drop_out_rate)\n",
    "                    for _ in range(num_expert)]\n",
    "    tower_units = [num_neu for _ in range(PUF_list.n_pufs)]\n",
    "\n",
    "\n",
    "    # Set up MMoE layer\n",
    "    mmoe_layers = MMoE(\n",
    "        units=units_mmoe,\n",
    "        use_gate_bias=True,\n",
    "        experts=experts_list,\n",
    "        num_tasks=PUF_list.n_pufs,\n",
    "        expert_activation=activation,\n",
    "        gate_activation=gate_activation,\n",
    "        dropout_rate=0.1,\n",
    "    )(input_layer)\n",
    "\n",
    "\n",
    "    output_layers = []\n",
    "    output_info = ['r'+str(i) for i in range(1,PUF_list.n_pufs+1)]\n",
    "    train_losses = {output: [] for output in output_info}\n",
    "    train_accuracies = {output: [] for output in output_info}\n",
    "    val_losses = {output: [] for output in output_info}\n",
    "    val_accuracies = {output: [] for output in output_info}\n",
    "\n",
    "    # Build tower layer from MMoE layer\n",
    "    for index, task_layer in enumerate(mmoe_layers):\n",
    "        tower_layer = Dense(\n",
    "            units=tower_units[index],\n",
    "            activation=activation)(task_layer)\n",
    "        output_layer = Dense(\n",
    "            units=1,\n",
    "            name=output_info[index],\n",
    "            activation='sigmoid',\n",
    "            kernel_initializer=kernel_init)(tower_layer)\n",
    "        output_layers.append(output_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[input_layer], outputs=output_layers)\n",
    "    optimizer = 'adam'\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer = optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    SingleDynamic_Threshold = SingleDynamicThresholdCallback(gate_activation[0])\n",
    "    # Print out model architecture summary\n",
    "    model.summary()\n",
    "    batch_size = min(N_train//40,20000)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_c,\n",
    "        y=train_r_groups[0],\n",
    "        validation_data=(test_c, test_r_groups[0]),\n",
    "        batch_size=batch_size,\n",
    "        epochs=2000\n",
    "        ,callbacks = [memory_callback,SingleDynamic_Threshold]\n",
    "\n",
    "    )\n",
    "    return model, history,output_info,train_losses,train_accuracies,val_losses,val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Only need to choose which PUF and how many CRPs, do not need to change the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed= 922\n",
      "CRP seed= 100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "PUF_list = PUFs(stages=64)\n",
    "PUF_list.seed = random.randint(1,1000)\n",
    "print(\"seed=\",PUF_list.seed)\n",
    "CRP_seed = random.randint(1,100)\n",
    "print(\"CRP seed=\",CRP_seed)\n",
    "\n",
    "N_train = 3000000\n",
    "# Choose one PUF to be model\n",
    "PUF_list.add_XOR_PUF(k=7,num=1)\n",
    "\n",
    "\n",
    "## Below can be uncommented as your will\n",
    "# PUF_list.add_XOR_PUF(k=2,num=1)\n",
    "# PUF_list.add_XOR_PUF(k=3,num=1)\n",
    "# PUF_list.add_XOR_PUF(k=4,num=1)\n",
    "# PUF_list.add_XOR_PUF(k=5,num=1)\n",
    "# PUF_list.add_XOR_PUF(k=6,num=1)\n",
    "# PUF_list.add_FF_PUF([(32,50)],1)\n",
    "# PUF_list.add_herero_XORFF_PUFs(k=2,ff=[[(20,50),(19,52)],[(21,54),(22,55)],[(30,60),(31,62)]],num=1)\n",
    "# PUF_list.add_herero_XORFF_PUFs(k=3,ff=[[(20,50)],[(21,54)],[(30,60)]],num=1)\n",
    "# PUF_list.add_interpose_PUFs(1,5,1)\n",
    "# PUF_list.add_interpose_PUFs(2,2,1)\n",
    "# PUF_list.add_interpose_PUFs(3,3,1)\n",
    "# PUF_list.add_XORFF_PUF(2,[(20,50)],1)\n",
    "# PUF_list.add_XORFF_PUF(4,[(20,50),(21,54)],1)\n",
    "# PUF_list.add_XORFF_PUF(3,[(20,50),(21,54)],1)\n",
    "# PUF_list.add_XORFF_PUF(2,[(21,54)],1)\n",
    "# PUF_list.add_XORFF_PUF(2,[(21,54),(10,40)],1)\n",
    "# PUF_list.add_XORFF_PUF(1,[(20,50),(10,40),(30,60)],1)\n",
    "# PUF_list.add_XOR_PUF(1,num=1)\n",
    "\n",
    "[c, responses] = PUF_list.generate_crps(CRP_seed,N_train)\n",
    "c = get_parity_vectors2(c)\n",
    "# c = np.concatenate([c,get_parity_vectors2(c)],axis=1)\n",
    "responses = np.array(responses)\n",
    "# print(responses.shape)\n",
    "train_c,test_c,train_r,test_r = train_test_split(c,responses.T,test_size=0.2, random_state=42)\n",
    "train_r_groups = [train_r[:,i].reshape(-1,1) for i in range(PUF_list.n_pufs)]\n",
    "test_r_groups = [test_r[:,i].reshape(-1,1) for i in range(PUF_list.n_pufs)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a demo of using real data. The only thing you need to do is changing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'data/4To9_XPUF_Silicon_CRPs/7XOR_64bit_LUT_2239B_attacking_5M.txt'\n",
    "# path = 'data/7puf_output_2000000.txt'\n",
    "# data = pd.read_csv(path,sep=';',header=None)\n",
    "# challenges = np.asarray(data[0].apply(lambda x: bin(int(x, 16))[2:].zfill(64)))\n",
    "# responses = []\n",
    "# for k in range(7):\n",
    "#     responses.append(np.asarray(data[1])//(10**k)%10)\n",
    "# response_XOR2 = responses[1] ^ responses[2]\n",
    "# challenges = np.array([[int(bit) for bit in ch] for ch in challenges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "m_mo_e (MMoE)                [(None, 32)]              22408     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "r1 (Dense)                   (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 23,497\n",
      "Trainable params: 23,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
      "120/120 [==============================] - 3s 16ms/step - loss: 0.6988 - accuracy: 0.5003 - val_loss: 0.6937 - val_accuracy: 0.5005\n",
      "Epoch 2/2000\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.6935 - accuracy: 0.5006 - val_loss: 0.6934 - val_accuracy: 0.5008\n",
      "Epoch 3/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6933 - val_accuracy: 0.5008\n",
      "Epoch 4/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.5008\n",
      "Epoch 5/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 6/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6931 - accuracy: 0.5029 - val_loss: 0.6933 - val_accuracy: 0.5004\n",
      "Epoch 7/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6932 - val_accuracy: 0.4997\n",
      "Epoch 8/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6933 - val_accuracy: 0.4994\n",
      "Epoch 9/2000\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.5001\n",
      "Epoch 10/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6930 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5007\n",
      "Epoch 11/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6930 - accuracy: 0.5061 - val_loss: 0.6933 - val_accuracy: 0.5001\n",
      "Epoch 12/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6930 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.4995\n",
      "Epoch 13/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6929 - accuracy: 0.5072 - val_loss: 0.6934 - val_accuracy: 0.4992\n",
      "Epoch 14/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.5004\n",
      "Epoch 15/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6934 - val_accuracy: 0.4998\n",
      "Epoch 16/2000\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.6928 - accuracy: 0.5093 - val_loss: 0.6934 - val_accuracy: 0.5002\n",
      "Epoch 17/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6928 - accuracy: 0.5102 - val_loss: 0.6934 - val_accuracy: 0.5004\n",
      "Epoch 18/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5007\n",
      "Epoch 19/2000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.6926 - accuracy: 0.5108 - val_loss: 0.6935 - val_accuracy: 0.5008\n",
      "Epoch 20/2000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6934 - val_accuracy: 0.5015\n",
      "Epoch 21/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6925 - accuracy: 0.5127 - val_loss: 0.6934 - val_accuracy: 0.5017\n",
      "Epoch 22/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6924 - accuracy: 0.5131 - val_loss: 0.6934 - val_accuracy: 0.5019\n",
      "Epoch 23/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6935 - val_accuracy: 0.5006\n",
      "Epoch 24/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6934 - val_accuracy: 0.5030\n",
      "Epoch 25/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6921 - accuracy: 0.5145 - val_loss: 0.6934 - val_accuracy: 0.5028\n",
      "Epoch 26/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6934 - val_accuracy: 0.5026\n",
      "Epoch 27/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6931 - val_accuracy: 0.5054\n",
      "Epoch 28/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6913 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5057\n",
      "Epoch 29/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6907 - accuracy: 0.5196 - val_loss: 0.6920 - val_accuracy: 0.5080\n",
      "Epoch 30/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6899 - accuracy: 0.5208 - val_loss: 0.6904 - val_accuracy: 0.5106\n",
      "Epoch 31/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6878 - accuracy: 0.5253 - val_loss: 0.6858 - val_accuracy: 0.5199\n",
      "Epoch 32/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6825 - accuracy: 0.5324 - val_loss: 0.6764 - val_accuracy: 0.5313\n",
      "Epoch 33/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6720 - accuracy: 0.5432 - val_loss: 0.6641 - val_accuracy: 0.5450\n",
      "Epoch 34/2000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.6600 - accuracy: 0.5544 - val_loss: 0.6516 - val_accuracy: 0.5580\n",
      "Epoch 35/2000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.6476 - accuracy: 0.5661 - val_loss: 0.6400 - val_accuracy: 0.5689\n",
      "Epoch 36/2000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.6368 - accuracy: 0.5755 - val_loss: 0.6317 - val_accuracy: 0.5777\n",
      "Epoch 37/2000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.6291 - accuracy: 0.5831 - val_loss: 0.6252 - val_accuracy: 0.5854\n",
      "Epoch 38/2000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.6228 - accuracy: 0.5907 - val_loss: 0.6188 - val_accuracy: 0.5938\n",
      "Epoch 39/2000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.6160 - accuracy: 0.5997 - val_loss: 0.6116 - val_accuracy: 0.6027\n",
      "Epoch 40/2000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.6094 - accuracy: 0.6079 - val_loss: 0.6044 - val_accuracy: 0.6126\n",
      "Epoch 41/2000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.6012 - accuracy: 0.6182 - val_loss: 0.5940 - val_accuracy: 0.6254\n",
      "Epoch 42/2000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.5893 - accuracy: 0.6321 - val_loss: 0.5806 - val_accuracy: 0.6419\n",
      "Epoch 43/2000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.5752 - accuracy: 0.6489 - val_loss: 0.5643 - val_accuracy: 0.6627\n",
      "Epoch 44/2000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.5593 - accuracy: 0.6677 - val_loss: 0.5482 - val_accuracy: 0.6794\n",
      "Epoch 45/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.5428 - accuracy: 0.6856 - val_loss: 0.5322 - val_accuracy: 0.6953\n",
      "Epoch 46/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.5279 - accuracy: 0.7005 - val_loss: 0.5175 - val_accuracy: 0.7089\n",
      "Epoch 47/2000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.5121 - accuracy: 0.7145 - val_loss: 0.5018 - val_accuracy: 0.7233\n",
      "Epoch 48/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4972 - accuracy: 0.7284 - val_loss: 0.4880 - val_accuracy: 0.7370\n",
      "Epoch 49/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4841 - accuracy: 0.7400 - val_loss: 0.4755 - val_accuracy: 0.7476\n",
      "Epoch 50/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4720 - accuracy: 0.7507 - val_loss: 0.4646 - val_accuracy: 0.7569\n",
      "Epoch 51/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4603 - accuracy: 0.7609 - val_loss: 0.4526 - val_accuracy: 0.7673\n",
      "Epoch 52/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4486 - accuracy: 0.7706 - val_loss: 0.4425 - val_accuracy: 0.7759\n",
      "Epoch 53/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4384 - accuracy: 0.7786 - val_loss: 0.4317 - val_accuracy: 0.7844\n",
      "Epoch 54/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4290 - accuracy: 0.7870 - val_loss: 0.4235 - val_accuracy: 0.7924\n",
      "Epoch 55/2000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.4185 - accuracy: 0.7947 - val_loss: 0.4119 - val_accuracy: 0.7998\n",
      "Epoch 56/2000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.4086 - accuracy: 0.8022 - val_loss: 0.4020 - val_accuracy: 0.8071\n",
      "Epoch 57/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.3984 - accuracy: 0.8097 - val_loss: 0.3921 - val_accuracy: 0.8146\n",
      "Epoch 58/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3886 - accuracy: 0.8169 - val_loss: 0.3815 - val_accuracy: 0.8224\n",
      "Epoch 59/2000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.3780 - accuracy: 0.8242 - val_loss: 0.3702 - val_accuracy: 0.8290\n",
      "Epoch 60/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3684 - accuracy: 0.8310 - val_loss: 0.3599 - val_accuracy: 0.8358\n",
      "Epoch 61/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3582 - accuracy: 0.8365 - val_loss: 0.3513 - val_accuracy: 0.8407\n",
      "Epoch 62/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3477 - accuracy: 0.8433 - val_loss: 0.3419 - val_accuracy: 0.8466\n",
      "Epoch 63/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3383 - accuracy: 0.8492 - val_loss: 0.3341 - val_accuracy: 0.8512\n",
      "Epoch 64/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3318 - accuracy: 0.8524 - val_loss: 0.3261 - val_accuracy: 0.8553\n",
      "Epoch 65/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3234 - accuracy: 0.8577 - val_loss: 0.3188 - val_accuracy: 0.8597\n",
      "Epoch 66/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3159 - accuracy: 0.8614 - val_loss: 0.3114 - val_accuracy: 0.8644\n",
      "Epoch 67/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.3087 - accuracy: 0.8656 - val_loss: 0.3056 - val_accuracy: 0.8675\n",
      "Epoch 68/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.3030 - accuracy: 0.8689 - val_loss: 0.2987 - val_accuracy: 0.8717\n",
      "Epoch 69/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2948 - accuracy: 0.8737 - val_loss: 0.2920 - val_accuracy: 0.8752\n",
      "Epoch 70/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2885 - accuracy: 0.8773 - val_loss: 0.2851 - val_accuracy: 0.8793\n",
      "Epoch 71/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.2825 - accuracy: 0.8805 - val_loss: 0.2804 - val_accuracy: 0.8813\n",
      "Epoch 72/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2768 - accuracy: 0.8834 - val_loss: 0.2747 - val_accuracy: 0.8849\n",
      "Epoch 73/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.2722 - accuracy: 0.8860 - val_loss: 0.2691 - val_accuracy: 0.8874\n",
      "Epoch 74/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.2677 - accuracy: 0.8885 - val_loss: 0.2648 - val_accuracy: 0.8899\n",
      "Epoch 75/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2627 - accuracy: 0.8913 - val_loss: 0.2606 - val_accuracy: 0.8920\n",
      "Epoch 76/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2578 - accuracy: 0.8942 - val_loss: 0.2557 - val_accuracy: 0.8947\n",
      "Epoch 77/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2541 - accuracy: 0.8958 - val_loss: 0.2514 - val_accuracy: 0.8973\n",
      "Epoch 78/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2487 - accuracy: 0.8986 - val_loss: 0.2471 - val_accuracy: 0.8995\n",
      "Epoch 79/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2446 - accuracy: 0.9007 - val_loss: 0.2433 - val_accuracy: 0.9014\n",
      "Epoch 80/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.2403 - accuracy: 0.9030 - val_loss: 0.2381 - val_accuracy: 0.9039\n",
      "Epoch 81/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2359 - accuracy: 0.9055 - val_loss: 0.2344 - val_accuracy: 0.9059\n",
      "Epoch 82/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2324 - accuracy: 0.9070 - val_loss: 0.2315 - val_accuracy: 0.9072\n",
      "Epoch 83/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2291 - accuracy: 0.9088 - val_loss: 0.2269 - val_accuracy: 0.9096\n",
      "Epoch 84/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.2254 - accuracy: 0.9105 - val_loss: 0.2232 - val_accuracy: 0.9117\n",
      "Epoch 85/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2225 - accuracy: 0.9119 - val_loss: 0.2195 - val_accuracy: 0.9129\n",
      "Epoch 86/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2183 - accuracy: 0.9140 - val_loss: 0.2175 - val_accuracy: 0.9144\n",
      "Epoch 87/2000\n",
      "120/120 [==============================] - 4s 31ms/step - loss: 0.2162 - accuracy: 0.9149 - val_loss: 0.2159 - val_accuracy: 0.9153\n",
      "Epoch 88/2000\n",
      "120/120 [==============================] - 4s 31ms/step - loss: 0.2125 - accuracy: 0.9169 - val_loss: 0.2115 - val_accuracy: 0.9172\n",
      "Epoch 89/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2110 - accuracy: 0.9176 - val_loss: 0.2094 - val_accuracy: 0.9181\n",
      "Epoch 90/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2074 - accuracy: 0.9192 - val_loss: 0.2055 - val_accuracy: 0.9199\n",
      "Epoch 91/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2045 - accuracy: 0.9207 - val_loss: 0.2041 - val_accuracy: 0.9208\n",
      "Epoch 92/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.2022 - accuracy: 0.9215 - val_loss: 0.2012 - val_accuracy: 0.9225\n",
      "Epoch 93/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.1998 - accuracy: 0.9229 - val_loss: 0.1978 - val_accuracy: 0.9239\n",
      "Epoch 94/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1961 - accuracy: 0.9246 - val_loss: 0.1953 - val_accuracy: 0.9252\n",
      "Epoch 95/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.1947 - accuracy: 0.9251 - val_loss: 0.1928 - val_accuracy: 0.9256\n",
      "Epoch 96/2000\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 0.1919 - accuracy: 0.9265 - val_loss: 0.1909 - val_accuracy: 0.9268\n",
      "Epoch 97/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1903 - accuracy: 0.9273 - val_loss: 0.1889 - val_accuracy: 0.9277\n",
      "Epoch 98/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1873 - accuracy: 0.9287 - val_loss: 0.1870 - val_accuracy: 0.9287\n",
      "Epoch 99/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1855 - accuracy: 0.9295 - val_loss: 0.1844 - val_accuracy: 0.9300\n",
      "Epoch 100/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1826 - accuracy: 0.9306 - val_loss: 0.1822 - val_accuracy: 0.9310\n",
      "Epoch 100 - Memory Cost: 6993.92 MB\n",
      "Epoch 101/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1813 - accuracy: 0.9313 - val_loss: 0.1802 - val_accuracy: 0.9319\n",
      "Epoch 102/2000\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 0.1792 - accuracy: 0.9323 - val_loss: 0.1784 - val_accuracy: 0.9328\n",
      "Epoch 103/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1771 - accuracy: 0.9331 - val_loss: 0.1776 - val_accuracy: 0.9332\n",
      "Epoch 104/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1755 - accuracy: 0.9341 - val_loss: 0.1744 - val_accuracy: 0.9346\n",
      "Epoch 105/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1734 - accuracy: 0.9349 - val_loss: 0.1738 - val_accuracy: 0.9350\n",
      "Epoch 106/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1717 - accuracy: 0.9360 - val_loss: 0.1715 - val_accuracy: 0.9360\n",
      "Epoch 107/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1691 - accuracy: 0.9370 - val_loss: 0.1701 - val_accuracy: 0.9365\n",
      "Epoch 108/2000\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 0.1676 - accuracy: 0.9375 - val_loss: 0.1677 - val_accuracy: 0.9374\n",
      "Epoch 109/2000\n",
      "120/120 [==============================] - 3s 29ms/step - loss: 0.1660 - accuracy: 0.9382 - val_loss: 0.1670 - val_accuracy: 0.9381\n",
      "Epoch 110/2000\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 0.1644 - accuracy: 0.9389 - val_loss: 0.1643 - val_accuracy: 0.9389\n",
      "Epoch 111/2000\n",
      "120/120 [==============================] - 4s 30ms/step - loss: 0.1622 - accuracy: 0.9401 - val_loss: 0.1629 - val_accuracy: 0.9399\n",
      "Epoch 112/2000\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 0.1608 - accuracy: 0.9406 - val_loss: 0.1606 - val_accuracy: 0.9408\n",
      "Epoch 113/2000\n",
      "120/120 [==============================] - 4s 33ms/step - loss: 0.1593 - accuracy: 0.9414 - val_loss: 0.1602 - val_accuracy: 0.9410\n",
      "Epoch 114/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.1580 - accuracy: 0.9422 - val_loss: 0.1591 - val_accuracy: 0.9417\n",
      "Epoch 115/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.1571 - accuracy: 0.9424 - val_loss: 0.1579 - val_accuracy: 0.9420\n",
      "Epoch 116/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1567 - accuracy: 0.9427 - val_loss: 0.1560 - val_accuracy: 0.9427\n",
      "Epoch 117/2000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.1554 - accuracy: 0.9433 - val_loss: 0.1541 - val_accuracy: 0.9440\n",
      "Epoch 118/2000\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.1525 - accuracy: 0.9445 - val_loss: 0.1537 - val_accuracy: 0.9443\n",
      "Epoch 119/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1524 - accuracy: 0.9443 - val_loss: 0.1530 - val_accuracy: 0.9445\n",
      "Epoch 120/2000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1506 - accuracy: 0.9449 - val_loss: 0.1507 - val_accuracy: 0.9451\n",
      "Epoch 121/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1507 - accuracy: 0.9452 - val_loss: 0.1499 - val_accuracy: 0.9455\n",
      "Epoch 122/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1489 - accuracy: 0.9459 - val_loss: 0.1489 - val_accuracy: 0.9457\n",
      "Epoch 123/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1481 - accuracy: 0.9462 - val_loss: 0.1477 - val_accuracy: 0.9462\n",
      "Epoch 124/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1463 - accuracy: 0.9468 - val_loss: 0.1472 - val_accuracy: 0.9467\n",
      "Epoch 125/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1454 - accuracy: 0.9473 - val_loss: 0.1455 - val_accuracy: 0.9471\n",
      "Epoch 126/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1445 - accuracy: 0.9478 - val_loss: 0.1446 - val_accuracy: 0.9472\n",
      "Epoch 127/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1439 - accuracy: 0.9479 - val_loss: 0.1442 - val_accuracy: 0.9478\n",
      "Epoch 128/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1426 - accuracy: 0.9486 - val_loss: 0.1437 - val_accuracy: 0.9482\n",
      "Epoch 129/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1422 - accuracy: 0.9487 - val_loss: 0.1433 - val_accuracy: 0.9485\n",
      "Epoch 130/2000\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 0.1408 - accuracy: 0.9494 - val_loss: 0.1417 - val_accuracy: 0.9491\n",
      "Epoch 131/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1404 - accuracy: 0.9493 - val_loss: 0.1414 - val_accuracy: 0.9489\n",
      "Epoch 132/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1395 - accuracy: 0.9499 - val_loss: 0.1396 - val_accuracy: 0.9494\n",
      "Epoch 133/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1388 - accuracy: 0.9501 - val_loss: 0.1394 - val_accuracy: 0.9498\n",
      "Early Stop!\n",
      "Memory cost: 6.57 GiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "units_mmoe = 32\n",
    "units_tower = 16\n",
    "model = None\n",
    "import pandas as pd\n",
    "######################################################################\n",
    "gate_activation = [CustomSoftmaxThre(threshold=0.00001) for _ in range(PUF_list.n_pufs)]\n",
    "# Below is the normal softmax activation.\n",
    "# gate_activation = [activations.get('softmax')]\n",
    "\n",
    "\n",
    "model,history, output_info, train_losses, train_accuracies, val_losses, val_accuracies = Model_Multiple_PUFs(\n",
    "    units_mmoe=units_mmoe,\n",
    "    gate_activation=gate_activation,\n",
    "    N_train=N_train,\n",
    "    train_c=train_c,\n",
    "    test_c=test_c,\n",
    "    train_r_groups=train_r_groups,\n",
    "    test_r_groups=test_r_groups,\n",
    "    PUF_list=PUF_list,\n",
    "    units_tower=units_tower)\n",
    "\n",
    "memory_info_end = process.memory_info() \n",
    "\n",
    "# print memory cost\n",
    "print(f\"Memory cost: {(memory_info_end.rss- memory_info_start.rss) / (1024 ** 3):.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show one example of modelling 7-XOR APUF. For this case, the training is very fast, we also admit that it may increase for different random seeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
